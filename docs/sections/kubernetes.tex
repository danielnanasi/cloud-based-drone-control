\chapter{Kialakított Kubernetes alapú felhő}
\label{cha:kubernetes}

Ebben a fejezetben rátérünk a felhőrendszer tényleges megvalósítására. A \ref{cha:cloud}. fejezetben megnéztük milyen lehetséges mai technológiák közül választhatunk, realizálhattuk, hogy a Kubernetes tűnik a legjobb választásnak ilyen célra, most pedig ezen a vonalon haladunk tovább. Megnézzük Kubernetesen belül milyen lehetőségek vannak, mik a probléma alapfeltételei és hogyan lehet integrálni a \ref{cha:fizikai}. fejezetben bemutatott konténerkollaborációt egy ilyen Kubernetes felhőbe.

\section{Kubernetes technológiái}
Több Kubernetes technológia közül választhatunk, bepillantunk némelyikbe, hogy mire jó és miért ezt választjuk vagy nem választjuk.

\subsection{K8S}

A K8s a Kubernetes rövidítése ("K", majd 8 "ubernete", majd "s" betű). Azonban általában, amikor az emberek Kubernetesről vagy K8-ról beszélnek, akkor az eredeti upstream projektről beszélnek, amelyet a Google valóban rendkívül elérhető és skálázható platformként tervezett.

Tehát a Kubernetes minden alapfunkcióval, mely összességének tulajdonságai:
\begin{itemize}
	\item elválasztott Master és Worker node-ok, biztosítható az irányítás erőforrása
	\item etcd külön clusteren futtatható, biztosítható a terhelés kezelése
	\item ideális esetben külön bejáratú csomópontokkal rendelkezik, hogy azok könnyedén kezeljék a bejövő forgalmat, még akkor is, ha az alatta lévő csomópontok némelyike foglalt. \cite{k8svsk3s}
\end{itemize}
\subsection{K3S}
A K3S egy egyszerűsített változata a K8S-nek, melynek forrása 40MB bináris fájl, amely teljesen implementálja a Kubernetes API-t. Rengeteg extra driver-t kihagytak belőle, melyre alap esetben nincs szükség tesztrendszer vagy egyszerű klaszter esetén. Ezeket a kihagyott funkciókat egyébként később hozzá lehet illeszteni a rendszerhez add-onokkal. \cite{k8svsk3s}
\subsection{Kind}

A Kind egy Docker fölötti Kubernetes megvalósítás egy node-on. Egyszerű installálni, azonban nem a Kubernetes API-t használja.

\subsection{MiniKube}

A MiniKube az első Kubernetes technológia amely a fejlesztők ajánlása alapján a kezdőknek kipróbálásra a legalkalmasabb. Mivel egyszerű telepíteni, nincs nagy erőforrásigénye (2 vCPU/2GB RAM/20GB lemez). Egy gépre installálható, nem adható több node a klaszterhez. \cite{typesofkubernetes2}

\subsection{Miért K3S?}

A tanszéki klaszter természetesen egy teljes kialakított K8S, melyen az eredeti API használható és teljesértékű szolgáltatásokat lehet tesztelni a Kubernetes összes optimalizálásával. A Kind más API-t használ, így a telepítést leegyszerűsíteni, azonban nem összeegyeztethető egy Kind-os applikáció K8S megvalósításával. MiniKube már egyel jobb, azonban csak egy node-ot használ, ebben a projekben pedig fontos a hálózati tesztelés több node között. Így marad a K3S, amellyel a legjobban szimulálhatjuk a tanszéki K8S rendszert és a megvalósított applikáció is könnyen portolható.

\section{Földi scenario felkészítése a menedzselt felhőrendszerbe}

A felhasznált Docker konténereket néhány esetben változtatni kellett, ez csak a \emph{Dockerfile}-ra igaz, a forráskódok az eredeti esetben is működtek nem K3S rendszeren. Mindegyik konténerben volt egy kivétel, amely csak akkor engedte futtathatóvá tenni a konténert, ha az Docker rendszerben fut, ezt a \emph{[/.dockerenv} fájl létezésére vonatkozó feltétel. \\

\noindent
A PX4 szimuláció futtatószkripje (\emph{vke\_px4sim/docker-entrypoint.sh}) ugyan tartalmazza a beépített \emph{ROS\_IP} hirdetőcímet, amelyet a hálózati fejlesztés során többször átírtam, sikerült olyan végeredményre jutni, amelyben az eredeti konténer külső interfész IP-je maradhat. \\

\noindent
Az előkészített Roscore konténert a végleges verzióban nem használom, hiszen egyszerűbb volt az eredeti \emph{alpineros/alpine-ros:noetic-ros-core} publikus image-t megadni, amit a K8S API-ban tudtam testreszabni indított portszámmal és környezeti változókkal.

\section{Kubernetes virtualiztált telepítése Multipass VM-eken}

Egy több node-os klasztert valósítok meg virtualizáció fölött. Virtuális gépek létrehozására rengeteg program létezik, én a Multipass-t választottam. A Multipass egy letisztult VM menedzser Linux-ra, Windows-ra és MacOS-re, amellyel egy parancs indítani és törölni különálló VM-eket bármely CloudInit-et tartalmazó image-ről. Azért választottam ezt, mert könnyedén szkriptelhető a Klaszter törlése és felhúzása, mivel a fejlesztés során sokszor szeretném az alapbeállításról indítani a klasztert. \\

\noindent
Tehát írtam egy Bash scriptet, aminek segítségével létrehozok három VM-et és inicializálom rajtuk a K3S klasztert. Multipass VM-ek létrehozása Bash-ben egy-egy parancs (\ref{lst:mlaunch}. számú lista), melyek paramétereit természetesen egy config fájlból olvasok be, amiről még később szó lesz.
\begin{lstlisting}[caption={Multipass VM-ek létrehozása},label={lst:mlaunch}]
multipass launch --name master --cpus 2 --mem 2G --disk 2G
for w in "worker-1 worker-2"; do
	multipass launch --name $slave --cpus 2 --mem 2G --disk 30G
done
\end{lstlisting} 
\noindent
Létrehozás után a szkript kiadatja az Apt csomagkezelő program \emph{update és upgrade} parancsait, hogy a legfrissebb Ubuntu 20.04 kompatibilis csomagok legyenek a VM-eken. \\

\noindent
Frissítés után a master VM-en inicializálható a K3S master módban. A parancs eleje azt mutatja, hogy a master nevű VM-en hajtjük végre a "--" utáni parancsokat (\ref{lst:minit}. lista).
\begin{lstlisting}[caption={K3S Master inicializálása},label={lst:minit}]
multipass exec master -- /bin/bash -c "curl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE="644" sh -"
\end{lstlisting}
\noindent
Ha ez sikeres, akkor a worker node-okat is inicializálhatjuk, amihez két dologra lesz szükség, a master külső IP-jére, amin keresztül a másik VM tudja elérni, illetve a K3S egyedi tokenre. A master külső IP-jét a multipass egyik parancsából olvassuk ki, a grep programmal rákeresve a master névre, majd az IP cím formátumára reguláris kifejezéssel. A tokenhez szimplán kiíratunk egy fájlt a cat programmal. Ezekkel pedig a master-hez hasonló módon a K3S dokumentációban megadott curl programhívással inicializáljuk a slave-eket. (\ref{lst:initk3s}. lista)
\begin{lstlisting}[caption={K3S Slave-ek inicializálása},label={lst:initk3s}]
K3S_TOKEN=$(multipass exec $master sudo cat /var/lib/rancher/k3s/server/node-token)
MASTER_IP=$(multipass list | grep $master | grep -oE "\b([0-9]{1,3}\.){3}[0-9]{1,3}\b")

for slave in $slaves; do
	multipass exec $slave -- /bin/bash -c "curl -sfL https://get.k3s.io | K3S_TOKEN=${K3S_TOKEN} K3S_URL=https://${MASTER_IP}:6443 sh -"
done
\end{lstlisting}

\noindent
Sikerességet ellenőrizve a szkiptben még kiíratom a master-en a csatlakozott node-okat.
\begin{lstlisting}[caption={Node-ok lekérdezése}]
multipass exec $master kubectl get nodes
\end{lstlisting}

\section{Konténer registry, lokális és központi}
Docker-Compose esetén a konténer kollaborációs megvalósításhoz elég volt definiálni a könyvtárt, mely forrása és konfigurációja alapján meg kell építeni az image-et és a lokális Docker környezet eltárolta ezen image-eket, amit fel is tudott használni az adott környezetben. Ez természetesnek tűnt, azonban a Kubernetes alapvetően nem tartalmaz ilyen fejlesztéseket, csakis lefordított képfájlokat tud publikus vagy privát registry-ből letölteni. A Docker Registry egy olyan verziókezelő, amely felépített konténer képfájlokat tárol és állít rendelkezésre aktívan használó rendszereknek, mint például a Kubernetes vagy Docker Swarm. Három megoldást próbáltam ki, hogy az egyedi lemezképeket használhassam a felhőmben (\ref{fig:registry}. ábra).\\

\noindent
\paragraph{A)}
Elsőnek úgy gondoltam, hogy a legegyszerűbb megoldás az ha a K3S vagy egy Docker környezetben, amely a K3S master-én futna, inicializálok egy előre készített Docker registry szervert, arra feltöltöm a felépített konténerlemezeket és lokálisan eléri a Kubernetes master, deploy esetén. Tehát definiáltam egy pod-ot, amely az 5000-res porton elérhető a K3S rendszeren belül, a master-ről. A \emph{registry} nevű image-et adtam meg, így a hivatalos Docker registry került letöltésre. Push-olni sikerült konténert, azonban a Kubernetes API-ra definiált Deployment-el már nem sikerült pull-olni SSL biztonsági okok miatt. \\

\noindent
\paragraph{B)}
Ezután hagytam a felhőbe integrálást, a master node-ra telepítettem Dockert és Docker-Compose-t, majd a deploy-oltam a hivatalos oldalról a registry-t (\ref{lst:registryB}. lista). A konténerek push-olása pedig szintén a master node-ról történt (\ref{lst:localreg} lista). Ez a megoldás működött, azonban nem túl szép és optimális, mivel a master-en sokszor tárolódik egy konténer image (forrásként, Docker image-ként, Docker registry-ben, Kubernetes letöltött image-ként és példányosított konténerként).
\begin{lstlisting}[caption={Docker registry inicializálás a master docker környezetében},label={lst:registryB}]
multipass exec ${master} -- sudo docker run -d -p 5000:5000 --restart=always --name registry registry:2
\end{lstlisting}
\begin{lstlisting}[caption={Build és push lokál konténer registry-be},label={lst:localreg}]
for container in "commander px4sim aruco"; do
	multipass exec ${master} -- sudo docker build ${drone_control_dir}/vke_${container}/. -t localhost:5000/${container}
	multipass exec ${master} -- sudo docker push localhost:5000/${container}
done
\end{lstlisting}

\paragraph{C)}
Így inkább talán a legegyszerűbb megoldást választottam, igénybe vettem a hivatalos Docker Hub privát repository szolgáltatását, amely ugyan egy konténert biztosít ingyen, egy kis trükkel, mi szerint ugyanazt a konténert töltöm fel 3 különböző taggel, ezt is meg lehet oldani (\ref{lst:registryC}. lista).
\begin{lstlisting}[caption={Docker Hub build és push},label={lst:registryC}]
for container in "commander px4sim aruco"; do
	docker build ${drone_control_dir}/vke_${container}/. -t nanasidnl/drone_control:${container}
	docker push nanasidnl/drone_control:${container}
done
\end{lstlisting}

 \begin{figure}
 	\centering
 	\includegraphics[width=\linewidth]{figures/registry.png}
 	\caption{Registry megoldásaim egyedi konténerek esetén}
 	\label{fig:registry}
 \end{figure}

\noindent
A Docker Hub privát repository-hoz authentikálni kell az API-t, ahhoz hogy elérje az adott K8S deployment. Ehhez egy configurációs fájlba kiszerveztem a hitelesítő adatokat. A szolgáltatást létrehozó deploy szkriptbe pedig beleírtam, hogy használja fel ezt a fájlt és authentikáljon egy \emph{regcred} titkon keresztül (\ref{lst:dhk3s}. lista). Ezt meg kell adni az API használatakor is (\ref{lst:dhsec}. lista).
\begin{lstlisting}[caption={Docker Hub authentikáció K3S-ről},label={lst:dhk3s}]
source ../config/docker-credentials.sh
multipass exec ${master} -- kubectl create secret docker-registry regcred --docker-server=${docker_server} --docker-username=${docker_username} --docker-password=${docker_password} --docker-email=${docker_email}
\end{lstlisting}

\begin{lstlisting}[caption={K8S API secret definiálása a konténerekhez},label={lst:dhsec}]
imagePullSecrets:
- name: regcred
\end{lstlisting}


\chapter{Drónirányítás mint szolgáltatás a Kubernetes felhőben}

Az előző (\ref{cha:kubernetes}.) fejezetben megmutattam, hogyan alakítottam ki a saját felhőrendszeremet. Ebben a fejezetben pedig megnézzük a diplomamunkának szánt feladat miként tud felkerülni ebbe a felhőbe.

\section{Első, 4 podos megvalósítása a szolgáltatásnak}
Elsőként a legegyszerűbb összeállításban szerettem volna megbizonyosodni arról, hogy a szolgáltatás valóban tud működni a felhőben ezért elsőnek a négy konténert 4 különálló pod-ként próbáltam ki (\ref{lst:1pod4}. lista). \\

\noindent
A Pod a legkisebb telepíthető számítási egység, amely létrehozható és kezelhető a Kubernetesben. Egy Pod létrehozása esetén példányosodik az előre definiált konténer vagy konténer csoport, azonban a futás megkezdése után semmilyen felügyeleti szolgáltatásban nem részesül, nem indul újra leállás esetén, nem alkalmazódik rá semmilyen skálázás. Egyszerűen ha csak Pod szinten futtatunk alkalmazásokat, akkor ugyanott járunk mintha Docker Swarm környezetben dolgoznánk. \\
\begin{lstlisting}[caption={Példa egy Pod-ra a négyből},label={lst:1pod4}]
apiVersion: v1
kind: Pod
metadata:
	name: aruco
	labels:
		name: drone-hq
spec:
	hostname: aruco
	containers:
	- 	image: nanasidnl/drone_control:aruco
		name: aruco
	env:
	- 	name: ROS_MASTER_URI
		value: "http://roscore:11311"
	- 	name: ARUCO_LAUNCH
		value: "sim"
	imagePullSecrets:
	-	name: regcred
\end{lstlisting}

\noindent
Jelen négy Podos megvalósításban a belső hálózati problémák nem merültek fel, hiszen a Pod neve hostname-ként is funkcionál, így egymás között könnyedén el tudták érni egymást az alkalmazások. Ez egy működő verzió volt, azonban túl egyszerű. A drón szimulációt nem a felhőben képzeljük el, hiszen a fizikai drón sem a felhőben fog futni.

\section{Konténerek hálózata, service-deployment 1 podos megoldás}

\section{Load Balancer, NodePort, Ingress}
https://medium.com/google-cloud/kubernetes-nodeport-vs-loadbalancer-vs-ingress-when-should-i-use-what-922f010849e0

\section{ROS Port forwarding}
https://journals.sagepub.com/doi/pdf/10.1177/1729881417703355

\section{Kauzalitási probléma, deploy sorrend}

\section{N drónra K3S service és drónok deployolása}

\section{N drón irányítása K3S-ből}
